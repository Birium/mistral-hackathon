<git_changes>

<file path="core/agent/agent/base_agent.py" status="modified">
import json
-from typing import List
+from typing import List, Generator
 
 from llm.client import LLMClient
 from llm.config import ModelConfig
 
 
 class BaseAgent:
+    """
+    Core agentic loop for Knower.
+
+    Runs an LLM in a tool-calling loop: the model reasons, calls tools,
+    receives results, and loops until it produces a final text answer
+    (i.e. a response with no tool_calls).
+
+    The agent is pure logic ‚Äî it yields event dicts and never prints.
+    Display is the caller's responsibility (terminal, API, etc.).
+
+    Events follow the schemas defined in schemas/event.py:
+      - "think"   : reasoning tokens from the model
+      - "answer"  : streamed text content, or tool_calls wrapper
+      - "tool"    : tool start / end / error with name, args, result
+      - "usage"   : token counts and cost for one LLM call
+      - "error"   : LLM-level errors
+    """
+
     def __init__(self, model: ModelConfig, system_prompt: str, tools: List = None):
         self.llm = LLMClient(
             model=model,
             tools=tools or [],
         )
 
-    def run(self, content: str) -> str:
-        """Run the agent loop on a single user input, return final answer."""
+    def run(self, content: str) -> Generator[dict, None, None]:
         messages = [HumanMessage(content=content)]
-        return self._loop(messages)
+        yield from self._loop(messages)
 
-    def _loop(self, messages: List[Message]) -> str:
-        """
-        N-round loop:
-        1. Call LLM ‚Äî may trigger tool calls (executed inline by LLMClient)
-        2. If tools were called, add results to conversation and loop again
-        3. If no tools were called, the agent has finished its task.
-        """
+    def _loop(self, messages: List[Message]) -> Generator[dict, None, None]:
         max_iterations = 15
         iteration = 0
-        
+
         while iteration < max_iterations:
             iteration += 1
-            full_answer = self._stream_and_collect(messages)
+
+            for event_json in self.llm.stream(messages):
+                yield json.loads(event_json)
 
             tool_calls = self.llm.get_tool_calls()
-            
-            # If the LLM didn't call any tools, it means it decided to answer directly.
-            # The loop ends here.
+
             if not tool_calls:
-                return full_answer
+                return
 
-            # Append assistant message (with tool_calls) + tool results to history
             messages.append(self.llm.get_full_response())
             for tc in tool_calls:
                 messages.append(ToolMessage(
                     content=tc.result_str or "",
                 ))
 
-        print("\n[Warning] Max agent iterations reached. Forcing stop.", flush=True)
-        return full_answer
-
-    def _stream_and_collect(self, messages: List[Message]) -> str:
-        """Stream one LLM call, display events, return accumulated text answer."""
-        full_answer = ""
-        for event_json in self.llm.stream(messages):
-            event = json.loads(event_json)
-            self._display(event)
-            # Collect plain text answer (not the tool_calls wrapper)
-            if event["type"] == "answer" and not event.get("tool_calls"):
-                full_answer += event.get("content", "")
-        return full_answer
-
-    def _display(self, event: dict):
-        """Print events to terminal in a readable way."""
-        t = event.get("type")
-
-        if t == "answer" and not event.get("tool_calls"):
-            print(event.get("content", ""), end="", flush=True)
-
-        elif t == "think":
-            # Affiche la r√©flexion en gris clair
-            print(f"\033[90m{event.get('content', '')}\033[0m", end="", flush=True)
-
-        elif t == "tool":
-            status = event.get("status")
-            name = event.get("name", "")
-            if status == "start":
-                print(f"\n  üîß {name}...", flush=True)
-            elif status == "end":
-                preview = str(event.get("result", ""))[:80].replace("\n", " ")
-                print(f"  ‚úì {name}: {preview}...", flush=True)
-            elif status == "error":
-                print(f"  ‚úó {name}: {event.get('result', '')}", flush=True)
-
-        elif t == "error":
-            print(f"\n  ‚ùå {event.get('content', '')}", flush=True)
-
-        elif t == "usage":
-            print(
-                f"\n  [{event.get('prompt_tokens', 0)}‚Üí"
-                f"{event.get('completion_tokens', 0)} tokens | "
-                f"${event.get('total_cost', 0):.5f}]",
-                flush=True,
-            )
+        yield {"type": "error", "id": "max_iter", "content": "max agent iterations reached, stopping."}
</file>

<file path="core/agent/agent/search_agent.py" status="modified">
tools=[TreeTool, ReadTool],
         )
 
-    def process(self, query: str) -> str:
+    def process(self, query: str):
         vault_context = self._load_vault_context()
         payload = f"{vault_context}\n\n---\n\n{query}"
-        return self.run(payload)
+        yield from self.run(payload)
 
     def _load_vault_context(self) -> str:
         from tools.dummy_tools import read
</file>

<file path="core/agent/agent/update_agent.py" status="modified">
tools=[TreeTool, ReadTool],
         )
 
-    def process(self, content: str, inbox_ref: str = None) -> str:
+    def process(self, content: str, inbox_ref: str = None):
         vault_context = self._load_vault_context()
         payload = f"{vault_context}\n\n---\n\n{content}"
         if inbox_ref:
             payload += f"\n\ninbox_ref: {inbox_ref}"
-        return self.run(payload)
+        yield from self.run(payload)
 
     def _load_vault_context(self) -> str:
         from tools.dummy_tools import read
</file>

<file path="core/agent/llm/client.py" status="modified">
reasoning_effort: str = "low",
         api_key: str = None,
     ):
-        from env import env  # lazy import to avoid circular
+        from env import env
         self.client = OpenAI(
             base_url=model.base_url,
             api_key=api_key or env.OPENROUTER_API_KEY,
         self.tools = tools or []
         self.reasoning_effort = reasoning_effort
 
-        # State reset on each stream() call
         self.thinking: str = ""
         self.content: str = ""
         self.tool_calls: List[ToolCall] = []
         self.is_thinking_started: bool = False
 
     def stream(self, messages: List[Message]) -> Generator[str, None, None]:
-        """
-        Stream LLM responses as JSON event strings.
-        Handles tool calls inline ‚Äî executes them and appends results.
-        """
         self._reset_state()
+        usage_event = None
 
         try:
             all_messages = [SystemMessage(content=self.system_prompt)] + messages
                 if event:
                     if hasattr(event, "id"):
                         message_id = event.id
-                    yield json.dumps(event.model_dump())
+                    if isinstance(event, UsageEvent):
+                        usage_event = event
+                    else:
+                        yield json.dumps(event.model_dump())
 
-            # If tool calls were requested ‚Äî execute them and yield results
             if self.tool_calls:
-                # Emit the assistant message with tool_calls
                 tool_calls_data = [
                     {
                         "id": tc.tool_id,
                     AnswerEvent(id=message_id, content="", tool_calls=tool_calls_data).model_dump()
                 )
 
-                # Execute each tool and yield events
                 for tc in self.tool_calls:
                     yield from self._execute_tool(tc)
 
+            if usage_event:
+                yield json.dumps(usage_event.model_dump())
+
         except Exception as e:
             traceback.print_exc()
             yield json.dumps(
             )
 
     def _execute_tool(self, tc: ToolCall) -> Generator[str, None, None]:
-        """Find the matching tool, run it, yield events."""
         tool = next((t for t in self.tools if t.name == tc.name), None)
 
         yield json.dumps(
             )
 
     def _process_chunk(self, chunk) -> Optional[BaseEvent]:
-        """Parse one streaming chunk into an event."""
         message_id = getattr(chunk, "id", "id_")
 
-        # Usage chunk
         if hasattr(chunk, "usage") and chunk.usage:
             cost = self.model.calculate_cost(
                 chunk.usage.prompt_tokens, chunk.usage.completion_tokens
 
         delta = chunk.choices[0].delta
 
-        # Thinking / reasoning
         if hasattr(delta, "reasoning") and delta.reasoning:
             thinking_chunk = delta.reasoning
             self.thinking += thinking_chunk
                 thinking_chunk = f"<think>\n{thinking_chunk}"
             return ThinkEvent(content=thinking_chunk, id=message_id)
 
-        # Close thinking block if we were thinking
         if self.is_thinking_started and not (hasattr(delta, "reasoning") and delta.reasoning):
             self.is_thinking_started = False
             return ThinkEvent(content="\n</think>\n", id=message_id)
 
-        # Text content
         if hasattr(delta, "content") and delta.content:
             self.content += delta.content
             return AnswerEvent(content=delta.content, id=message_id)
 
-        # Tool calls
         if hasattr(delta, "tool_calls") and delta.tool_calls:
             for tc_delta in delta.tool_calls:
-                # Extend tool_calls list if needed
                 while len(self.tool_calls) <= tc_delta.index:
                     self.tool_calls.append(
                         ToolCall(
</file>

<file path="core/agent/terminal.py" status="modified">
import sys
 import os
 
-# Add the parent directory (core/) to sys.path so we can import 'env' and other root modules
 sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 from agent.update_agent import UpdateAgent
 from agent.search_agent import SearchAgent
+from agent.display import Display
 
 
 def main():
-    print("üß† Knower")
+    print("Knower")
     print("Mode: [u]pdate / [s]earch ? ", end="")
     mode = input().strip().lower()
 
         agent = SearchAgent()
         label = "Search"
 
-    print(f"\n‚úì {label} agent ready.")
-    print("Ctrl+C to quit.\n")
+    print(f"\n{label} agent ready. Ctrl+C to quit.\n")
     print("-" * 50)
 
     while True:
         try:
-            user_input = input("\n‚Üí ").strip()
+            user_input = input("\nUser: ").strip()
             if not user_input:
                 continue
-            print()
-            agent.process(user_input)
-            print("\n" + "-" * 50)
+
+            display = Display()
+            for event in agent.process(user_input):
+                display.event(event)
+
+            print("\n\n" + "-" * 50)
 
         except KeyboardInterrupt:
             print("\n\nBye.")
             sys.exit(0)
         except Exception as e:
-            print(f"\n‚ùå {e}")
+            print(f"\nError: {e}")
 
 
 if __name__ == "__main__":
</file>

<file path="core/agent/agent/display.py" status="new">
"""
Terminal display for agent events.

Consumes the event stream from agents and formats everything
for human-readable terminal output. The agent itself never prints ‚Äî
this is the only place where events become visible text.

Events handled: answer, think, tool (start/end/error), usage, error.
"""

import json


def _format_tool_args(args_str: str) -> str:
    if not args_str:
        return ""
    try:
        args = json.loads(args_str)
        if not args:
            return ""
        return ", ".join(f'{k}="{v}"' for k, v in args.items())
    except (json.JSONDecodeError, TypeError):
        return args_str


def _indent_result(result: str) -> str:
    if not result:
        return "   (empty)"
    lines = result.splitlines()
    return "\n".join(f"   {line}" for line in lines)


class Display:
    def __init__(self):
        self.agent_started = False

    def event(self, data: dict):
        t = data.get("type")

        if t == "answer" and not data.get("tool_calls"):
            if not self.agent_started:
                print("\nAgent: ", end="", flush=True)
                self.agent_started = True
            print(data.get("content", ""), end="", flush=True)

        elif t == "think":
            print(f"\033[90m{data.get('content', '')}\033[0m", end="", flush=True)

        elif t == "tool":
            status = data.get("status")
            name = data.get("name", "")
            args_str = data.get("arguments", "")

            if status == "start":
                formatted = _format_tool_args(args_str)
                print(f"\nToolCall -> {name}({formatted})", flush=True)
            elif status == "end":
                result = str(data.get("result", ""))
                print(f"ToolResult:\n```\n{_indent_result(result)}\n```", flush=True)
            elif status == "error":
                print(f"ToolError -> {name}: {data.get('result', '')}", flush=True)

        elif t == "error":
            print(f"\nError: {data.get('content', '')}", flush=True)

        elif t == "usage":
            prompt = data.get("prompt_tokens", 0)
            completion = data.get("completion_tokens", 0)
            cost = data.get("total_cost", 0)
            print(f"\nTokens: [{prompt} in / {completion} out | ${cost:.5f}]", flush=True)
</file>

</git_changes>