# Checkpoint Brainstorm — Project Manager / Knowledge Agent
*Suite de la session du 22 fév 2026 — Session 4*

---

## Ce qui a changé par rapport aux sessions précédentes

Ce document est la suite directe des trois premiers brainstormings. Tout ce qui était tranché reste tranché. Ce document capture uniquement les évolutions, nouveaux éléments, et décisions prises dans cette session. Les quatre documents ensemble forment la spec complète.

---

## Décisions prises dans cette session

### 1. Pas d'archivage — ni pour les tasks, ni pour les changelogs

La question de la politique d'archivage est définitivement fermée. Il n'y a pas d'archivage.

**Tasks.md est une vue live exclusivement.** Il ne contient que ce qui est actif — les tâches en cours et à faire. Quand une tâche est complétée, elle disparaît de tasks.md. Elle n'est pas archivée dans un fichier séparé, elle n'est pas déplacée dans une section "Fait" qui grossit indéfiniment. Elle devient un événement dans le changelog : "Tâche X complétée." C'est son existence historique. Tasks.md reste donc lean en permanence, par design, sans intervention manuelle.

**Le changelog n'est jamais archivé.** Il peut faire 100 000 tokens, il peut faire plus — ça n'a pas d'importance. La raison pour laquelle l'archivage semblait nécessaire dans les sessions précédentes, c'est qu'on imaginait le charger entièrement en contexte. Ce n'est pas comme ça qu'il fonctionne. On ne le lit jamais en entier. On cherche dedans avec BM25 et le pipeline deep, on charge les chunks pertinents, et on répond. La structure H1 par jour, H2 par entrée fait que QMD peut le chunker proprement quelle que soit sa taille. Le changelog est en fait le meilleur candidat du vault pour grossir indéfiniment — c'est un log structuré, prévu pour ça, et les outils de search sont exactement conçus pour ce type de fichier.

La conséquence pratique : le changelog devient la mémoire historique complète du projet. L'évolution des tâches, les décisions, les événements, les pivots — tout. Tasks.md est une projection du présent, le changelog est l'enregistrement du passé. Les deux sont séparés conceptuellement et physiquement.

---

### 2. Search par date — paramètres additionnels sur le search tool existant

La recherche temporelle n'est pas un outil séparé. C'est deux paramètres optionnels ajoutés au search tool déjà défini : `date_from` et `date_to`.

Ces paramètres fonctionnent comme des filtres appliqués avant la recherche textuelle. Sur les changelogs, QMD filtre sur les H1 de date. Sur les fichiers bucket et les autres fichiers indexés, il filtre sur les champs `created` et `updated` du frontmatter YAML. Ensuite seulement, il applique BM25 ou le pipeline complet sur le sous-ensemble filtré.

Ce qui est puissant dans cette combinaison, c'est qu'elle se compose naturellement avec le scope et le mode. "Toutes les décisions prises sur startup-x en juin" devient `search("[décision]", mode: "fast", scope: "project:startup-x", date_from: "2025-06-01", date_to: "2025-06-30")`. "Tout ce qui s'est passé cette semaine" devient `search("*", date_from: "7 days ago")` sans scope, en full vault. La recherche temporelle n'est pas une feature à part — c'est une dimension de la recherche existante.

---

### 3. Scopes cross-cutting — naviguer par type de fichier à travers tous les projets

Le paramètre `scope` du search tool s'étend au-delà du projet individuel. En plus de `scope: "project:startup-x"` pour cibler un projet et `scope: "bucket"` pour les buckets, on ajoute des scopes qui traversent tous les projets en ciblant un type de fichier spécifique.

`scope: "all-states"` cherche dans tous les `projects/*/state.md`. `scope: "all-changelogs"` cherche dans tous les `projects/*/changelog.md` plus le `changelog.md` global. `scope: "all-tasks"` cherche dans tous les `projects/*/tasks.md` plus le `tasks.md` global. `scope: "all-buckets"` cherche dans tous les `projects/*/bucket/*` plus le `bucket/` global. `scope: "all-descriptions"` cherche dans tous les `projects/*/description.md`.

Sans scope, la search couvre l'intégralité du vault indexé.

Ces scopes résolvent un problème concret : quand la question est cross-projet et porte sur un type d'information spécifique, l'agent n'a pas à naviguer projet par projet. "Quels projets sont bloqués ?" → `search("bloqué", mode: "fast", scope: "all-states")`. "Qu'est-ce que j'avais décidé sur l'architecture de paiement ?" → `search("paiement architecture", mode: "deep", scope: "all-changelogs")`. Résultats précis, navigation évitée.

---

### 4. Read partiel — `head` et `tail` comme paramètres du read tool

Le read tool s'enrichit de deux paramètres optionnels : `head: N` et `tail: N`, où N est exprimé en tokens.

`read(path, head: 2000)` retourne les 2 000 premiers tokens du fichier. `read(path, tail: 2000)` retourne les 2 000 derniers. Sans paramètre, le read est complet comme avant.

L'utilité principale concerne les changelogs. Puisqu'ils sont newest-first (append top), `head: 2000` sur un changelog retourne les entrées les plus récentes — exactement ce que l'orchestrateur veut voir pour calibrer une nouvelle entrée sans charger 80 000 tokens d'historique. La décision de faire un read partiel est informée par tree.md : l'agent voit que le changelog fait 45 000 tokens, il sait qu'il n'a pas besoin de tout lire, il fait un `head: 2000` pour avoir le contexte récent et il append.

---

### 5. Le move tool — manquant jusqu'ici, maintenant intégré

`move(from, to)` déplace un fichier d'un emplacement à un autre dans le vault. C'est une opération filesystem pure, pas de lecture ni de réécriture de contenu.

Cas d'usage typiques : un fichier inbox après confirmation de routing est déplacé vers le bucket du projet concerné. Un fichier bucket global rattaché rétrospectivement à un projet est déplacé dans le bucket de ce projet. Un fichier mal rangé initialement est recorrrigé.

Le background job se déclenche après chaque move : il met à jour tree.md, ré-indexe le fichier dans QMD à son nouvel emplacement, et met à jour le champ `updated` dans le frontmatter. Si un fichier est déplacé dans le bucket d'un projet, le background job met automatiquement à jour le frontmatter du fichier pour y ajouter l'information de projet. L'orchestrateur ne gère pas ce détail — c'est de la plomberie déterministe.

La liste complète des tools est maintenant : **read** (avec head/tail optionnels), **write**, **edit**, **append**, **delete**, **move**, **tree** (callable dynamiquement), **search** (fast/deep, scope, date_from, date_to).

---

### 6. Types de fichiers acceptés — texte et images uniquement pour le MVP

Le système accepte du texte, des images, et des PDFs convertis en texte. Les fichiers audio passent par un processeur upstream qui les convertit en texte transcrit avant d'arriver au système. Tout ce qui n'est pas texte ou image est traité par des processeurs externes, en dehors du périmètre du service. Cette liste peut grandir avec le temps — il y aura toujours un processeur pour convertir ce qui arrive. À l'entrée du système, tout est soit du texte, soit une image.

---

### 7. Architecture deux tiers — orchestrateur cher, worker cheap

L'orchestrateur est le cerveau du système. Il tourne sur un modèle puissant — Claude Opus, ou équivalent — parce que ses tâches sont de la compréhension sémantique profonde, de la détection de contradictions, du routing complexe, et de la prise de décisions qui impactent la cohérence globale de l'arborescence. Il ne génère presque pas de texte long. Son output ce sont des décisions et des instructions vers le worker.

Le worker est le bras d'exécution. Il tourne sur un modèle cheap et rapide — Gemini Flash, ou équivalent — parce que sa tâche est mécanique : écrire une entrée de changelog, mettre à jour une section de state.md, reformuler un bloc de tasks.md. Ces tâches ne demandent pas de raisonnement sur la cohérence globale du vault. Elles demandent de bien écrire un bloc de texte précis dans un format connu.

La règle de dispatch est simple : tout ce qui est opération filesystem pure (move, delete, read) est exécuté directement par l'orchestrateur. Tout ce qui necessite de la génération de contenu markdown — écrire une entrée changelog, réécrire un state, créer une nouvelle tâche — est délégué au worker sous forme d'une micro-tâche avec son contexte.

---

### 8. Le contexte du worker — complet, jamais dégradé

C'est un point critique. Le worker doit recevoir l'intégralité du contexte disponible. Il reçoit l'input original de l'utilisateur, les fichiers joints, le texte du message — exactement comme l'orchestrateur les a reçus. Il reçoit aussi les fichiers que l'orchestrateur a lus pendant sa phase de raisonnement : le state.md du projet concerné, le head du changelog, le tasks.md si pertinent. Et il reçoit l'instruction spécifique de l'orchestrateur sur ce qu'il doit produire.

Un worker avec un contexte appauvri produit des entrées génériques, déconnectées du ton, du vocabulaire, et de la continuité de ce qui existe déjà dans les fichiers. Et une mémoire remplie de contenu générique et déconnecté, c'est précisément le problème que tout ce système est censé résoudre. Le coût d'un contexte riche pour le worker est négligeable — le modèle est cheap et rapide, les quelques milliers de tokens d'input supplémentaires ne changent rien économiquement. Ce qui change, c'est la qualité du texte produit.

---

### 9. Les deux agents tournent sur le même niveau de modèle

L'agent de maintenance et le search agent utilisent tous les deux un modèle puissant. La tentation de faire tourner le search agent sur un modèle moins cher parce qu'il "ne fait que chercher" est une erreur. La synthèse cross-projet — assembler des chunks de changelog, de state, de bucket venant de multiples projets pour répondre à une question complexe — est du raisonnement difficile. Un modèle downgraded produit des réponses qui manquent les connexions importantes, oublient des nuances, et ratent des contradictions entre sources. Le search agent reste sur un modèle cher.

---

### 10. Architecture MCP — local et cloud, même interface, juste une URL

Le service expose un MCP server. Il propose exactement deux opérations vers l'extérieur : envoyer de l'information (update) et chercher du contexte (search). C'est l'interface du système vers tous les clients — Claude Code, Cursor, un agent custom, un script, n'importe quel client compatible MCP.

En local, le server tourne sur localhost. Tout outil compatible qui tourne sur la même machine s'y connecte. En cloud, le même server est exposé sur une URL publique avec une couche d'authentification devant. Le code du server ne change pas entre les deux modes — c'est une question de transport et d'URL, rien d'autre.

Le vault de fichiers vit localement. Si l'utilisateur veut de la persistance cloud ou accéder à sa mémoire depuis des outils distants, il sync le dossier — git, rsync, n'importe quel mécanisme de sync de fichiers. Le service ne gère pas le sync lui-même. Sa responsabilité s'arrête aux fichiers tels qu'ils sont, là où ils sont. C'est le même principe qu'Obsidian : la source de vérité ce sont les fichiers locaux, la sync vers le cloud est de l'infrastructure optionnelle en dehors du produit.

Ce qui est puissant dans cette architecture, c'est que la mémoire est une et les portes d'entrée sont multiples. Claude Code au boulot appelle le MCP local. Un agent custom sur un serveur distant appelle le MCP en cloud. Les deux lisent et écrivent dans le même vault, via la même interface, avec les mêmes deux opérations.

---

### 11. Code execution comme tool — optionnel, non-prioritaire, très puissant

L'idée : au lieu d'exposer des tools individuels, on expose un seul tool d'exécution de code. L'agent écrit un script Python, le service l'exécute dans un environnement contrôlé qui a accès aux fonctions du vault (read, search, write, tree), et retourne le résultat en markdown.

Ce que ça change concrètement : l'agent peut combiner plusieurs opérations en un seul appel. Un `tree` suivi d'un `search` suivi de deux `read` ciblés devient un script qui fait les quatre en séquence et retourne un résultat assemblé. Moins d'allers-retours, moins de tokens de coordination, plus de précision sur les opérations complexes.

Ce n'est pas obligatoire pour le MVP. Avec du tool calling classique, le système fonctionne. Mais c'est un path d'optimisation identifié — en particulier pour les opérations de maintenance complexes qui enchaînent plusieurs reads et writes. Le modèle mental de l'agent ne change pas. Seule la couche d'exécution change. C'est donc une migration possible sans refonte, quand le besoin s'en fait sentir.

---

### 12. L'inbox et le mécanisme de reply — en suspens, à définir

Le flux d'entrée de l'inbox est clair : l'agent détecte une ambiguïté de routing, crée un fichier dans `inbox/`, met à jour `inbox/overview.md`, et notifie l'utilisateur via un canal externe (email, Discord, Telegram — peu importe le canal au stade de prototype).

Ce qui n'est pas encore défini, et qui est important, c'est **le flux de retour**. Quand l'utilisateur reçoit la notification — "Je ne sais pas où ranger ce vocal, ça ressemble à une tâche globale ou au projet appart-search ?" — il faut qu'il puisse répondre. Pas en ouvrant le fichier inbox manuellement et en le modifiant. En répondant directement depuis le canal de notification : un reply à l'email, un message dans le thread Discord, une réponse SMS si c'est SMS.

Le système capte cette réponse, l'agent de maintenance récupère le fichier inbox concerné et l'instruction de routing de l'utilisateur, traite la donnée, met à jour les fichiers impactés, supprime le fichier inbox, et met à jour `inbox/overview.md`.

Ce loop bidirectionnel — notification push vers l'utilisateur, réponse de l'utilisateur captée et traitée — est central à l'expérience inbox. Sans lui, l'inbox devient un silo passif que l'utilisateur doit aller consulter manuellement, et l'intérêt s'effondre. La mécanique précise de capture de la réponse (webhook de Discord, parsing d'un reply email, endpoint HTTP simple) n'est pas encore tranchée. C'est un sujet à part entière pour une session future.

---

## Questions ouvertes — mise à jour

**Le mécanisme de reply de l'inbox (PRIORITÉ HAUTE)**
C'est le seul point de friction volontaire du système. La mécanique de notification est posée (webhook externe, canal au choix). Ce qui reste à définir c'est le flux de retour : comment l'utilisateur répond, comment la réponse est captée, comment l'agent de maintenance traite la réponse et ferme la boucle. À traiter dans une session dédiée.

**Code execution tool**
Identifié comme path d'optimisation future. Non-prioritaire pour le MVP. À revisiter quand les besoins de performance de l'exécution se feront sentir concrètement.

**Le flow end-to-end du maintenance agent** reste formellement non tracé de façon exhaustive. Les éléments sont là — l'arbre de décision a été décrit, l'ordre d'opération est posé — mais un tracé complet "input reçu → tous les fichiers mis à jour → confirmation envoyée" avec les cas limites n'a pas encore été fait. Toujours prioritaire pour une session future.

**La politique d'archivage** est définitivement fermée comme question. La réponse est : pas d'archivage.

**L'interface, le MCP en cloud, et les intégrations avancées** restent des sujets futurs, post-MVP.

---

## Ce qui est maintenant tranché — résumé express

| Sujet | Décision |
|---|---|
| Archivage des tasks | Supprimé — tasks.md est une vue live uniquement |
| Archivage des changelogs | Supprimé — un seul fichier, croît indéfiniment, searchable |
| Tasks complétées | Disparaissent de tasks.md, capturées comme événements dans le changelog |
| Search par date | Paramètres `date_from` / `date_to` sur le search tool existant |
| Scopes cross-cutting | `all-states`, `all-changelogs`, `all-tasks`, `all-buckets`, `all-descriptions` |
| Read partiel | Paramètres `head: N` et `tail: N` en tokens sur le read tool |
| Move tool | Validé — opération filesystem pure, background job met à jour le reste |
| Types de fichiers | Texte et images uniquement — PDF et audio convertis en amont |
| Architecture deux tiers | Orchestrateur cher (raisonnement) + worker cheap (génération de contenu) |
| Contexte du worker | Complet et jamais dégradé — même input que l'orchestrateur + fichiers lus |
| Modèle du search agent | Même niveau que l'agent de maintenance — modèle puissant |
| MCP | Server local ou cloud, même interface, deux opérations (update / search) |
| Sync cloud | Infrastructure externe optionnelle — le service ne gère pas le sync |
| Code execution tool | Non-prioritaire MVP, path d'optimisation future identifié |
| Flux de return inbox | Important, non encore défini — à traiter en session dédiée |